"""
æä¾›ä¸¤ä¸ªæ¥å£ï¼š
1:reitsæœˆåº¦æ–‡ä»¶ä¸Šä¼ çŸ¥è¯†åº“
2:é€šè¿‡æ–‡ä»¶åè·å–ä¸Šä¼ æ–‡ä»¶çš„åˆ¶å®šæ•°æ®
"""

from contextlib import asynccontextmanager
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from fastapi.responses import JSONResponse
from fastapi import FastAPI
from pydantic import BaseModel
from langchain.chat_models import init_chat_model
from langchain.chains import RetrievalQA
from langchain.schema import Document

import pdfplumber
import asyncio
from langchain.agents import initialize_agent, AgentType
from pydantic import BaseModel, Field
from langchain_core.tools import tool
from mysql_tools import sql_inster

class QuestionRequest(BaseModel):
    file_name: str

# çŸ¥è¯†åº“åˆå§‹åŒ–å‡½æ•°
async def init_knowledge_base():
    def _sync_init():
        print("â³ æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“...")
        return Chroma(
            embedding_function=OllamaEmbeddings(model="nomic-embed-text"),
            persist_directory="./finance_kb"
        )
    return await asyncio.to_thread(_sync_init)

# 1. æå–PDFå†…å®¹ï¼ˆå«è¡¨æ ¼ï¼‰
def extract_pdf_content(pdf_path):
    text, tables = "", []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
            for table in page.extract_tables():
                markdown_table = "\n".join("| " + " | ".join(str(cell) for cell in row) + " |" for row in table)
                tables.append(f"è¡¨æ ¼ï¼š\n{markdown_table}\n")
    return text + "\n".join(tables)

# æ–‡ä»¶ä¸Šä¼ çŸ¥è¯†åº“
def upload_file(file_name):
    file_path = "./sh/2025-07-13/{}".format(file_name)
    # 2. åˆ†å—å¤„ç†
    content = extract_pdf_content(file_path)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200,
        separators=["\n\n", "\n", "è¡¨æ ¼ï¼š"]
    )
    chunks = text_splitter.split_text(content)

    documents = []
    # 3. ä¸ºæ¯ä¸ªchunkæ·»åŠ æ–‡ä»¶æ ‡è¯†å…ƒæ•°æ®
    for chunk in chunks:
        document = Document(page_content=chunk, metadata={"file_id": file_name})
        documents.append(document)

    # è¿½åŠ æ–°æ–‡æ¡£
    app.state.vector_store.add_documents(documents)

    return JSONResponse(content={
        "status": "success",
        "file_id": file_name,
        "chunk_count": len(documents)
    })      

llm = init_chat_model(model="qwen3:8b", model_provider="ollama")

# è·å–çŸ¥è¯†åº“ä¿¡æ¯
def get_parse_result(file_name):
    prompt = ChatPromptTemplate.from_template(
        """
        è¯·åˆ†æä»¥ä¸‹æ•°æ®ï¼ˆå«è¡¨æ ¼ï¼‰ï¼š
        ---
        {context}
        ---
        é—®é¢˜ï¼š{question}
        ï¼ˆå¦‚æ¶‰åŠè¡¨æ ¼ï¼Œè¯·ç”¨Markdownè¡¨æ ¼å›å¤ï¼‰
        """
        )
    
    # åˆ›å»ºå¸¦è¿‡æ»¤çš„æ£€ç´¢å™¨
    retriever = app.state.vector_store.as_retriever(
        search_kwargs={"filter": {"file_id": file_name}}
    )

    # æ„å»ºQAé“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},  # å…³é”®è®¾ç½®
        return_source_documents=True
    )
    
    questions = """
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ•°æ®åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»å…¬å‹ŸREITSå…¬å‘Šä¸­æå–å…³é”®è¿è¥æ•°æ®ã€‚
            è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ‰€æœ‰ç¬¦åˆè¦æ±‚çš„ä¿¡æ¯ï¼Œå¦‚æœæŸé¡¹ä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·å¡«å†™"null":
            éœ€è¦æå–çš„ä¿¡æ¯é¡¹å¦‚ä¸‹ï¼š
            1. æœˆä»½ (å¦‚æœé€‚ç”¨)
            2. å…¬å‹ŸREITSä»£ç 
            3. å…¬å‹ŸREITSç®€ç§°(ç®€ç§°ä¸æ˜¯å…¨ç§°ï¼Œæ¯”å¦‚å¹³å®‰å¹¿å·å¹¿æ²³REITï¼Œç®€ç§°æ˜¯å¹¿æ²³REIT)
            4. æ—¥å‡æ”¶è´¹è½¦æµé‡å½“æœˆ
            5. æ—¥å‡æ”¶è´¹è½¦æµé‡å½“æœˆç¯æ¯”å˜åŠ¨
            6. æ—¥å‡æ”¶è´¹è½¦æµé‡å½“æœˆåŒæ¯”å˜åŠ¨
            7. æ—¥å‡æ”¶è´¹è½¦æµé‡å¹´ç´¯è®¡
            8. æ—¥å‡æ”¶è´¹è½¦æµé‡ç´¯è®¡åŒæ¯”å˜åŠ¨
            9. è·¯è´¹æ”¶å…¥å½“æœˆ
            10. è·¯è´¹æ”¶å…¥å½“æœˆç¯æ¯”å˜åŠ¨
            11. è·¯è´¹æ”¶å…¥å½“æœˆåŒæ¯”å˜åŠ¨
            12. è·¯è´¹æ”¶å…¥å¹´ç´¯è®¡
            13. è·¯è´¹æ”¶å…¥ç´¯è®¡åŒæ¯”å˜åŠ¨

            **é‡è¦**: å›ç­”é—®é¢˜æ—¶å¿…é¡»æŒ‰ä»¥ä¸‹æ ¼å¼ï¼Œä¸è¦è¿”å›å…¶ä»–æ€è€ƒå†…å®¹ï¼›
            ```json
                {{
                    "month": "2024å¹´1æœˆ",
                    "reits_code": "ä»£ç ",
                    "reits_name": "ç®€ç§°",
                    "æ—¥å‡æ”¶è´¹è½¦æµé‡": {
                        "å½“æœˆ":"",
                        "å½“æœˆç¯æ¯”å˜åŠ¨":"",
                        "å½“æœˆåŒæ¯”å˜åŠ¨":"",
                        "å¹´ç´¯è®¡":"",
                        "ç´¯è®¡åŒæ¯”å˜åŠ¨":"",
                        "æ—¥å‡æ”¶è´¹è½¦æµé‡å•ä½":""
                    },
                    "è·¯è´¹æ”¶å…¥": {
                        "å½“æœˆ":"",
                        "å½“æœˆç¯æ¯”å˜åŠ¨":"",
                        "å½“æœˆåŒæ¯”å˜åŠ¨":"",
                        "å¹´ç´¯è®¡":"",
                        "ç´¯è®¡åŒæ¯”å˜åŠ¨":"",
                        "è·¯è´¹æ”¶å…¥å•ä½":""
                    }
                }}
            ```
            """
    return qa_chain({"query": questions})

tools = [sql_inster]

agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# ç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ­£ç¡®çš„å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ç°"""
    try:
        # å¼‚æ­¥åˆå§‹åŒ–
        print("ğŸŸ¢ æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“...")
        app.state.vector_store = await init_knowledge_base()
        # å…³é”®ç‚¹ï¼šå¿…é¡»æœ‰yield
        yield
    except Exception as e:
        print(f"ğŸš¨ åˆå§‹åŒ–å¤±è´¥: {e}")
        raise    

app = FastAPI(lifespan=lifespan)

@app.post("/upload/")
async def ask_question(request: QuestionRequest):
    return upload_file(request.file_name)


@app.post("/ask/")
async def ask_question(request: QuestionRequest):
    result = get_parse_result(request.file_name)

    # è°ƒç”¨Toolå­˜å‚¨ç»“æœ
    agent.run(
        f"å°†å¤§æ¨¡å‹è¾“å‡ºçš„ç»“æœresultï¼š{result['result']}å­˜å…¥æ•°æ®åº“"
    )

    return result


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)